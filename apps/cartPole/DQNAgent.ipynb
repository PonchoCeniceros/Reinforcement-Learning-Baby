{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b08698ac6ae40c3fec0fa4f4d8ec29e65affd5fea78266e6d171626ace241fa3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## **DQN Agent**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# no mostrar alertas de tensorflow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import deque"
   ]
  },
  {
   "source": [
    "### **Declaración del agente**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, environment, path, verbose=False):\n",
    "        \"\"\" Inicializar el agente\n",
    "\n",
    "        Args:\n",
    "            environment (gym.wrappers.time_limit.TimeLimit): ambiente en el que se desenvuelve el agente\n",
    "            path (string): ruta para almecenar/cargar la matriz de pesos para la NN\n",
    "            verbose (bool, optional): habilitar los comentarios. Defaults to False.\n",
    "        \"\"\"\n",
    "        # resguardamos el ambiente\n",
    "        self._env = environment\n",
    "        self._stateSize = self._env.observation_space.shape[0]\n",
    "        self._actionSize = self._env.action_space.n\n",
    "\n",
    "        # parámetros que decaerán con el tiempo\n",
    "        self._epsilon = 1.0\n",
    "        self._lr = 0.001\n",
    "        self._gamma = 0.95\n",
    "        # constantes para decaer los parámetros\n",
    "        self._minEpsilon = 0.1\n",
    "        self._minLr = 0.1\n",
    "        self._discount = 0.98\n",
    "        self._decay = 25\n",
    "\n",
    "        # reservamos memoria para almacenar los estados requeridos para el\n",
    "        # aprendizaje y generamos el modelo de NN que actuará como nuestra \"matriz Q\"\n",
    "        self._memory  = deque(maxlen=2000)\n",
    "        self._QValues = self._build_model()\n",
    "\n",
    "        # otras cosas que requerimos\n",
    "        self._path = path\n",
    "        self._verbose = verbose\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\" Se define el modelo de red neuronal que se empleará para el aprendizaje\n",
    "            del agente.\n",
    "\n",
    "        Returns:\n",
    "            keras.models.Sequential: modelo definido\n",
    "        \"\"\"\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Dense(5, input_dim=self._stateSize, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(10, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(self._actionSize, activation=\"linear\"))\n",
    "        model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=self._lr))\n",
    "        return model\n",
    "\n",
    "\n",
    "    def _decayEpsilon(self, t):\n",
    "        \"\"\" calcular el decaimiento del exploration rate\n",
    "\n",
    "        Args:\n",
    "            t (int): instante de tiempo transcurrido durante el apisodio\n",
    "        \"\"\"\n",
    "        self._epsilon = max(self._minEpsilon, min(1., 1. - math.log10((t + 1) / self._decay)))\n",
    "\n",
    "\n",
    "    def _mapState(self, state):\n",
    "        \"\"\" Transformar el estado continuo en un estado discreto mediante el empleo de buckets\n",
    "\n",
    "        Args:\n",
    "            state (tuple): el estado actual del ambiente\n",
    "\n",
    "        Returns:\n",
    "            list: el estado transformado\n",
    "        \"\"\"\n",
    "        return np.reshape(state, [1, self._stateSize])\n",
    "\n",
    "\n",
    "    def _capacity(self):\n",
    "        \"\"\" Entregar la capacidad actual de nuestro buffer de memoria\n",
    "\n",
    "        Returns:\n",
    "            int: capacidad actual de nuestro buffer de memoria\n",
    "        \"\"\"\n",
    "        return len(self._memory)\n",
    "\n",
    "\n",
    "    def _chooseAction(self, state):\n",
    "        \"\"\" Obtener la la mejor accion posible dado el estado del ambiente\n",
    "\n",
    "        Args:\n",
    "            state (numpy.ndarray): el estado actual del ambiente\n",
    "\n",
    "        Returns:\n",
    "            int: la mejor accion posible\n",
    "        \"\"\"\n",
    "        # aleatoriamente regresamos una acción aleatoria para ayudar al\n",
    "        # agente a que no caiga en un minimo local\n",
    "        if np.random.rand() <= self._epsilon:\n",
    "            return random.randrange(self._actionSize)\n",
    "        # retornamos la mejor accion posible\n",
    "        QValues = self._QValues.predict(state)\n",
    "        return np.argmax(QValues[0])\n",
    "\n",
    "\n",
    "    def _memorize(self, state, action, reward, newState, done):\n",
    "        \"\"\" Alamacenar en la memoria del agente el estado general del ambiente en\n",
    "            un instante del entrenamiento \n",
    "\n",
    "        Args:\n",
    "            state (numpy.ndarray): estado actual del ambiente\n",
    "            action (numpy.int64): acción realizada por el agente\n",
    "            reward (float): recompensa otorgada en base a la acción\n",
    "            newState (numpy.ndarray): estado nuevo a partir de la acción generada\n",
    "            done (bool): si el ambiente ha terminado su ejecución o no\n",
    "        \"\"\"\n",
    "        self._memory.append((state, action, reward, newState, done))\n",
    "\n",
    "\n",
    "    def _load(self, filename):\n",
    "        \"\"\" Cargar una matriz de pesos en el modelo\n",
    "\n",
    "        Args:\n",
    "            filename (string): nombre del archivo con la matriz de pesos\n",
    "        \"\"\"\n",
    "        self._QValues.load_weights(filename)\n",
    "\n",
    "\n",
    "    def _save(self, filename):\n",
    "        \"\"\" Guardar la matriz de pesos actual del modelo\n",
    "\n",
    "        Args:\n",
    "            filename (string): nombre del archivo con la matriz de pesos\n",
    "        \"\"\"\n",
    "        self._QValues.save_weights(filename)\n",
    "\n",
    "\n",
    "    def _learn(self, batchSize):\n",
    "        \"\"\" Entrenar el modelo en base a la memoria acumulada del agente\n",
    "\n",
    "        Args:\n",
    "            batchSize (int): porción de la memoria que se empleará para \n",
    "                             entrenar el modelo\n",
    "        \"\"\"\n",
    "        # se extraen un batch con los estados más recientes\n",
    "        batch = random.sample(self._memory, batchSize)\n",
    "\n",
    "        for state, action, reward, newState, done in batch:\n",
    "            # empleamos la recompensa del estado general como target para entrenar,\n",
    "            # esto en caso de que en dicho estado el ambiente siga activo. En caso contrario,\n",
    "            # se calcula la recompensa correspondiente según la ecuación de Bellman\n",
    "            target = reward if not done else reward + self._gamma*np.amax(self._QValues.predict(newState)[0])\n",
    "            # calculamos los Q-values con nuestro modelo actual\n",
    "            target_f = self._QValues.predict(state)\n",
    "            # reemplazamos el Q-value correspondiente a la acción generada en el estado general\n",
    "            # por la recompensa calculada\n",
    "            target_f[0][action] = target\n",
    "            # con los Q-values como target, entrenamos nuestro modelos\n",
    "            self._QValues.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "    def learn(self, episodes, timesteps, batchSize=32, save=False):\n",
    "            \"\"\" Someter al agente al proceso de aprendizaje\n",
    "\n",
    "            Args:\n",
    "                episodes (int): épocas de entrenamiento\n",
    "                timesteps (int): instantes de tiempo en los que el ambiente se entrenará/evaluará\n",
    "                batchSize (int): porción de la memoria que se empleará para entrenar el modelo. Defaults to 32.\n",
    "                save (bool): flag para indicar al agente si debe o no guardar los pesos. Defaults to False.\n",
    "            \"\"\"\n",
    "            # para cada epoca de entrenamiento\n",
    "            for episode in range(episodes):\n",
    "                # obtenemos el estado actual del ambiente y lo redimensionamos\n",
    "                state = self._env.reset()\n",
    "                state = self._mapState(state)\n",
    "\n",
    "                # decaer el exploration rate por episodio\n",
    "                self._decayEpsilon(episode)\n",
    "\n",
    "                # durante X pasos generaremos una acción sobre el ambiente\n",
    "                # y obtendremos una recompensa.\n",
    "                for t in range(timesteps):\n",
    "                    # AGENTE EJECUTA ACCIÓN DADO EL ESTADO ACTUAL\n",
    "                    action = self._chooseAction(state)\n",
    "                    # obtendremos el estado nuevo del ambiente ante el estimulo dado por el agente\n",
    "                    # y lo redimencionamos también\n",
    "                    newState, reward, done, _ = self._env.step(action)\n",
    "                    newState = self._mapState(newState)\n",
    "                    # ALMACENAMOS EN MEMORIA DEL AGENTE EL ESTADO GENERAL DEL AMBIENTE \n",
    "                    self._memorize(state, action, reward, newState, done)                \n",
    "                    # actualizamos el estado actual con el nuevo\n",
    "                    state = newState\n",
    "                    # si terminó la época, salimos\n",
    "                    if done:\n",
    "                        break\n",
    "                    # SI SE TIENEN LOS DATOS SUFICIENTES, ENTRENAR EL MODELO DEL AGENTE\n",
    "                    if self._capacity() > batchSize:\n",
    "                        self._learn(batchSize)\n",
    "                    # guardar la matriz de pesos cada ciertas épocas\n",
    "                    if save and episode%10 == 0:\n",
    "                        self._save(self._path)\n",
    "\n",
    "                # cerramos el ambiente\n",
    "                self._env.close()\n",
    "\n",
    "\n",
    "    def run(self, load=False):\n",
    "        \"\"\" Hacer que el agente interactue con el ambiente\n",
    "\n",
    "        Args:\n",
    "            load (bool): flag para indicar al agente si debe o no cargar los pesos. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            int: intantes de tiempo que duró el agente interactuando con el ambiente\n",
    "        \"\"\"\n",
    "        t = 0\n",
    "        done = False\n",
    "        # cargar la matriz de pesos en la NN\n",
    "        if load:\n",
    "            self._load(self._path)\n",
    "        # obtenemos el estado actual del ambiente y lo redimensionamos\n",
    "        state = self._env.reset()\n",
    "        state = self._mapState(state)\n",
    "\n",
    "        while not done:\n",
    "            self._env.render()\n",
    "            t = t + 1\n",
    "            # AGENTE EJECUTA ACCIÓN DADO EL ESTADO ACTUAL\n",
    "            action = self._chooseAction(state)\n",
    "            # obtendremos el estado nuevo del ambiente ante el estimulo dado por el agente\n",
    "            # y lo redimencionamos también\n",
    "            newState, reward, done, _ = self._env.step(action)\n",
    "            newState = self._mapState(newState)\n",
    "            # actualizamos el estado actual con el nuevo\n",
    "            state = newState\n",
    "\n",
    "        # cerramos el ambiente\n",
    "        self._env.close()\n",
    "        return t"
   ]
  },
  {
   "source": [
    "### **Instanciar el agente y entrenarlo**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos la ruta del folder donde guardaemos los pesos\n",
    "dirPath = os.path.join(os.path.abspath(''), 'data')\n",
    "# si no existe el folder, lo creamos\n",
    "if not os.path.exists(dirPath):\n",
    "    os.makedirs(dirPath, exist_ok=True)\n",
    "# observamos la ruta\n",
    "dirPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(environment=gym.make('CartPole-v0'), path=os.path.join(dirPath, 'cartpole-dqn.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn(episodes=10, timesteps=200, save=True)\n",
    "print('training finished:')"
   ]
  },
  {
   "source": [
    "### **Observar al agente en acción**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = agent.run(load=True)\n",
    "t"
   ]
  }
 ]
}