{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b08698ac6ae40c3fec0fa4f4d8ec29e65affd5fea78266e6d171626ace241fa3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## **SARSA Agent**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "### **Declaración del agente**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    def __init__(self, environment, verbose=False):\n",
    "        \"\"\" Inicializar el agente\n",
    "\n",
    "        Args:\n",
    "            environment (gym.wrappers.time_limit.TimeLimit): ambiente en el que se desenvuelve el agente\n",
    "            verbose (bool, optional): habilitar los comentarios. Defaults to False.\n",
    "        \"\"\"\n",
    "        # resguardamos el ambiente\n",
    "        self._env = environment\n",
    "        self._stateSize = self._env.observation_space.shape[0]\n",
    "        self._actionSize = self._env.action_space.n\n",
    "\n",
    "        # definimos tanto el número de buckets para cada valor de estado (x, y, Δx, Δy, θ, ω, cl, cr)\n",
    "        # así como sus límites superior e inferior\n",
    "        self._buckets = (5, 5, 5, 5, 5, 5, 2, 2)\n",
    "\n",
    "\n",
    "        # creamos una lista con los límites superior e inferior de cada dimensión\n",
    "        self._stateBounds = list(zip(self._env.observation_space.low, self._env.observation_space.high))\n",
    "        # se fijan los valores para cada dimensión\n",
    "        self._stateBounds[0] = [-1, 1] # x\n",
    "        self._stateBounds[1] = [-1, 1] # y\n",
    "        self._stateBounds[2] = [-1, 1] # Δx\n",
    "        self._stateBounds[3] = [-1, 1] # Δy\n",
    "        self._stateBounds[4] = [-1, 1] # θ\n",
    "        self._stateBounds[5] = [-1, 1] # ω\n",
    "        self._stateBounds[6] = [ 0, 1] # cl\n",
    "        self._stateBounds[7] = [ 0, 1] # cr\n",
    "\n",
    "        # reservamos memoria para almacenar los valores requeridos para\n",
    "        # determinar la política\n",
    "        self._QValues = np.zeros(self._buckets + (self._actionSize,))\n",
    "\n",
    "        # parámetros que decaerán con el tiempo\n",
    "        self._epsilon = 1.0\n",
    "        self._gamma = 0.95\n",
    "        self._lr = 1.0\n",
    "        # constantes para decaer los parámetros\n",
    "        self._minLr = 0.1\n",
    "        self._minEpsilon = 0.1\n",
    "        self._discount = 0.98\n",
    "        self._decay = 25\n",
    "\n",
    "        # otras cosas que requerimos\n",
    "        self._verbose = verbose\n",
    "\n",
    "\n",
    "    def _mapState(self, state):\n",
    "        \"\"\" Transformar el estado continuo en un estado discreto mediante el empleo de buckets\n",
    "\n",
    "        Args:\n",
    "            state (tuple): el estado actual del ambiente\n",
    "\n",
    "        Returns:\n",
    "            list: el estado transformado\n",
    "        \"\"\"\n",
    "        ratios = [ (state[i] + abs(self._stateBounds[i][0])) / (self._stateBounds[i][1] - self._stateBounds[i][0]) for i in range(len(state)) ]\n",
    "        mState = [ int(round((self._buckets[i] - 1) * ratios[i])) for i in range(len(state)) ]\n",
    "        mState = [ min(self._buckets[i] - 1, max(0, mState[i]))   for i in range(len(state)) ]\n",
    "        return tuple(mState)\n",
    "\n",
    "\n",
    "    def _decayEpsilon(self, t):\n",
    "        \"\"\" calcular el decaimiento del exploration rate\n",
    "\n",
    "        Args:\n",
    "            t (int): instante de tiempo transcurrido durante el apisodio\n",
    "        \"\"\"\n",
    "        self._epsilon = max(self._minEpsilon, min(1., 1. - math.log10((t + 1) / self._decay)))\n",
    "\n",
    "\n",
    "    def _decayLearningRate(self, t):\n",
    "        \"\"\" calcular el decaimiento del learning rate\n",
    "\n",
    "        Args:\n",
    "            t (int): instante de tiempo transcurrido durante el apisodio\n",
    "        \"\"\"\n",
    "        self._lr = max(self._minLr, min(1., 1. - math.log10((t + 1) / self._decay)))\n",
    "\n",
    "\n",
    "    def _chooseAction(self, state):\n",
    "        \"\"\" Obtener la la mejor accion posible dado el estado del ambiente\n",
    "\n",
    "        Args:\n",
    "            state (list): el estado actual del ambiente\n",
    "\n",
    "        Returns:\n",
    "            int: la mejor accion posible\n",
    "        \"\"\"\n",
    "        # aleatoriamente regresamos una acción aleatoria para ayudar al\n",
    "        # agente a que no caiga en un minimo local\n",
    "        if np.random.rand() <= self._epsilon:\n",
    "            return random.randrange(self._actionSize)\n",
    "        # retornamos la mejor accion posible\n",
    "        return np.argmax(self._QValues[state])\n",
    "\n",
    "\n",
    "\n",
    "    def _learn(self, state, newState, action, newAction, reward):\n",
    "        \"\"\" Actualizar los Q-values con la ecuación de Bellman\n",
    "\n",
    "        Args:\n",
    "            state (tuple):  estado actual del ambiente\n",
    "            newState ([type]): estado nuevo a partir de la acción generada\n",
    "            action ([type]): acción realizada por el agente\n",
    "            newAction ([type]): acción realizada por el agente a partir del nuevo estado\n",
    "            reward (float): recompensa otorgada en base a la acción\n",
    "        \"\"\"\n",
    "        # actualizar la matriz Q con la ecuación de Bellman\n",
    "        self._QValues[state][action] += self._lr*(reward + self._gamma*(self._QValues[newState][newAction] - self._QValues[state][action])) # np.max\n",
    "\n",
    "    def learn(self, episodes, timesteps):\n",
    "        \"\"\" Someter al agente al proceso de aprendizaje\n",
    "\n",
    "        Args:\n",
    "            episodes (int): épocas de entrenamiento\n",
    "            timesteps (int): instantes de tiempo en los que el ambiente se entrenará/evaluará\n",
    "        \"\"\"\n",
    "        # para cada epoca de entrenamiento\n",
    "        for episode in range(episodes):\n",
    "            # obtenemos el estado actual del ambiente y lo redimensionamos\n",
    "            state = self._env.reset()\n",
    "            state = self._mapState(state)\n",
    "\n",
    "            # decaer el exploration rate por episodio\n",
    "            self._decayEpsilon(episode)\n",
    "            self._decayLearningRate(episode)\n",
    "\n",
    "            # durante X pasos generaremos una acción sobre el ambiente\n",
    "            # y obtendremos una recompensa.\n",
    "            for t in range(timesteps):\n",
    "                # AGENTE EJECUTA ACCIÓN DADO EL ESTADO ACTUAL\n",
    "                action = self._chooseAction(state)\n",
    "                # obtendremos el estado nuevo del ambiente ante el estimulo dado por el agente\n",
    "                # y lo redimencionamos también\n",
    "                newState, reward, done, _ = self._env.step(action)\n",
    "                newState = self._mapState(newState)\n",
    "                # AGENTE EJECUTA ACCIÓN DADO EL NUEVO ESTADO\n",
    "                newAction = self._chooseAction(newState)\n",
    "\n",
    "                self._learn(state, newState, action, newAction, reward)\n",
    "                \n",
    "                # actualizamos el estado actual con el nuevo\n",
    "                state = newState\n",
    "                # si terminó la época, salimos\n",
    "                if done:\n",
    "                    break\n",
    "            # cerramos el ambiente\n",
    "            self._env.close()\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\" Hacer que el agente interactue con el ambiente\n",
    "\n",
    "        Returns:\n",
    "            int: intantes de tiempo que duró el agente interactuando con el ambiente\n",
    "        \"\"\"\n",
    "        t = 0\n",
    "        done = False\n",
    "        # obtenemos el estado actual del ambiente y lo redimensionamos\n",
    "        state = self._env.reset()\n",
    "        state = self._mapState(state)\n",
    "\n",
    "        while not done:\n",
    "            self._env.render()\n",
    "            t = t + 1\n",
    "            # AGENTE EJECUTA ACCIÓN DADO EL ESTADO ACTUAL\n",
    "            action = self._chooseAction(state)\n",
    "            # obtendremos el estado nuevo del ambiente ante el estimulo dado por el agente\n",
    "            # y lo redimencionamos también\n",
    "            newState, reward, done, _ = self._env.step(action)\n",
    "            newState = self._mapState(newState)\n",
    "            # actualizamos el estado actual con el nuevo\n",
    "            state = newState\n",
    "\n",
    "        # cerramos el ambiente\n",
    "        self._env.close()\n",
    "        return t"
   ]
  },
  {
   "source": [
    "### **Instanciar el agente y entrenarlo**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = SARSAAgent(environment=gym.make('LunarLander-v2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training finished:\n"
     ]
    }
   ],
   "source": [
    "agent.learn(episodes=1000, timesteps=200)\n",
    "print('training finished:')"
   ]
  },
  {
   "source": [
    "### **Observar al agente en acción**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "t = agent.run()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}