{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "e5b47908a3ad695ab55066e00d7f832a99edfd6ea3fa51bfd80a44b3b52ea06a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## **Bellman Q Value**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "```\n",
    "[\n",
    "    # estado S0\n",
    "    [   \n",
    "        [0.7, 0.3, 0.0], # acción A0\n",
    "        [1.0, 0.0, 0.0], # acción A1\n",
    "        [0.8, 0.2, 0.0]  # acción A2\n",
    "    ],\n",
    "    # estado S1\n",
    "    [\n",
    "        [0.0, 1.0, 0.0], # acción A0\n",
    "        None,            # acción A1\n",
    "        [0.0, 0.0, 1.0]  # acción A2\n",
    "    ],\n",
    "    # estado S2\n",
    "    [ \n",
    "        None,            # acción A0\n",
    "        [0.8, 0.1, 0.1], # acción A1\n",
    "        None             # acción A2\n",
    "    ]\n",
    "]\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitionProbs = [\n",
    "    [ [0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    [ [0.0, 1.0, 0.0], None,            [0.0, 0.0, 1.0]],\n",
    "    [ None,            [0.8, 0.1, 0.1], None           ]\n",
    "]\n",
    "\n",
    "np.array(transitionProbs).shape # shape = [s, a, s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [\n",
    "    [ [+10, 0, 0], [  0, 0, 0], [0, 0,  0]],\n",
    "    [ [  0, 0, 0], [  0, 0, 0], [0, 0,-50]],\n",
    "    [ [  0, 0, 0], [+40, 0, 0], [0, 0,  0]]\n",
    "]\n",
    "\n",
    "np.array(rewards).shape # shape = [s, a, s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possibleActions = [\n",
    "    [0, 1, 2], # estado S0\n",
    "    [0, 2], # estado S1\n",
    "    [1] # estado S2\n",
    "]\n",
    "\n",
    "QValues = np.full((3, 3), -np.inf) # -np.inf para todas las acciones imposibles\n",
    "\n",
    "for state, actions in enumerate(possibleActions):\n",
    "    QValues[state, actions] = 0.0 # for all possible actions\n",
    "\n",
    "QValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gamma = 1.0 # factor de descuento\n",
    "iters = 50 # número de iteraciones\n",
    "\n",
    "for _ in range(iters):\n",
    "    # los estados S actuales pasan a ser los S' anteriores\n",
    "    QPrev = QValues.copy()\n",
    "    # para el estado S\n",
    "    for s in range(3):\n",
    "        # para la acción A\n",
    "        for a in possibleActions[s]:\n",
    "            # evaluar la ecuación de Bellman en el estado S, dada la mejor acción A en S'\n",
    "            QValues[s, a] = np.sum( [ transitionProbs[s][a][sp]*(rewards[s][a][sp] + gamma*np.max(QPrev[sp])) for sp in range(3) ] )\n",
    "\n",
    "QValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(QValues,axis=1) # acción óptima para cada estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}